{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "region_proposal_object_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girijeshcse/computervisionTFKeras/blob/main/object_detection/03%20region_proposal_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAckSQgGaGHY"
      },
      "source": [
        "### Install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9FkaOfWwawu"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/region-proposal-object-detection/region-proposal-object-detection.zip\n",
        "!unzip -qq region-proposal-object-detection.zip\n",
        "%cd region-proposal-object-detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgTVT3HagGZ"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from imutils.object_detection import non_max_suppression\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBrLwCtN5kqy"
      },
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRw969Dp5Kdm"
      },
      "source": [
        "def plt_imshow(title, image):\n",
        "\t# convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jppw5-Bd56H-"
      },
      "source": [
        "### Implementing region proposal object detection with OpenCV, Keras, and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X71RWICdLE3h"
      },
      "source": [
        "def selective_search(image, method=\"fast\"):\n",
        "\t# initialize OpenCV's selective search implementation and set the\n",
        "\t# input image\n",
        "\tss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "\tss.setBaseImage(image)\n",
        "\n",
        "\t# check to see if we are using the *fast* but *less accurate* version\n",
        "\t# of selective search\n",
        "\tif method == \"fast\":\n",
        "\t\tss.switchToSelectiveSearchFast()\n",
        "\n",
        "\t# otherwise we are using the *slower* but *more accurate* version\n",
        "\telse:\n",
        "\t\tss.switchToSelectiveSearchQuality()\n",
        "\n",
        "\t# run selective search on the input image\n",
        "\trects = ss.process()\n",
        "\n",
        "\t# return the region proposal bounding boxes\n",
        "\treturn rects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okM7Bpyeq8Kc"
      },
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "#\thelp=\"path to the input image\")\n",
        "#ap.add_argument(\"-m\", \"--method\", type=str, default=\"fast\",\n",
        "#\tchoices=[\"fast\", \"quality\"],\n",
        "#\thelp=\"selective search method\")\n",
        "#ap.add_argument(\"-c\", \"--conf\", type=float, default=0.9,\n",
        "#\thelp=\"minimum probability to consider a classification/detection\")\n",
        "#ap.add_argument(\"-f\", \"--filter\", type=str, default=None,\n",
        "#\thelp=\"comma separated list of ImageNet labels to filter on\")\n",
        "#args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"image\": \"beagle.png\",\n",
        "\t\"method\": \"fast\",\n",
        "\t\"conf\": 0.9,\n",
        "\t\"filter\": \"beagle\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKxaph4NLQhi"
      },
      "source": [
        "# grab the label filters command line argument\n",
        "labelFilters = args[\"filter\"]\n",
        "\n",
        "# if the label filter is not empty, break it into a list\n",
        "if labelFilters is not None:\n",
        "\tlabelFilters = labelFilters.lower().split(\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBFkiIGfLScd"
      },
      "source": [
        "# load ResNet from disk (with weights pre-trained on ImageNet)\n",
        "print(\"[INFO] loading ResNet...\")\n",
        "model = ResNet50(weights=\"imagenet\")\n",
        "\n",
        "# load the input image from disk and grab its dimensions\n",
        "image = cv2.imread(args[\"image\"])\n",
        "(H, W) = image.shape[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S13Qg4pWLUSU"
      },
      "source": [
        "# run selective search on the input image\n",
        "print(\"[INFO] performing selective search with '{}' method...\".format(\n",
        "\targs[\"method\"]))\n",
        "rects = selective_search(image, method=args[\"method\"])\n",
        "print(\"[INFO] {} regions found by selective search\".format(len(rects)))\n",
        "\n",
        "# initialize the list of region proposals that we'll be classifying\n",
        "# along with their associated bounding boxes\n",
        "proposals = []\n",
        "boxes = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaH10q9RLWdE"
      },
      "source": [
        "# loop over the region proposal bounding box coordinates generated by\n",
        "# running selective search\n",
        "for (x, y, w, h) in rects:\n",
        "\t# if the width or height of the region is less than 10% of the\n",
        "\t# image width or height, ignore it (i.e., filter out small\n",
        "\t# objects that are likely false-positives)\n",
        "\tif w / float(W) < 0.1 or h / float(H) < 0.1:\n",
        "\t\tcontinue\n",
        "\n",
        "\t# extract the region from the input image, convert it from BGR to\n",
        "\t# RGB channel ordering, and then resize it to 224x224 (the input\n",
        "\t# dimensions required by our pre-trained CNN)\n",
        "\troi = image[y:y + h, x:x + w]\n",
        "\troi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
        "\troi = cv2.resize(roi, (224, 224))\n",
        "\n",
        "\t# further preprocess by the ROI\n",
        "\troi = img_to_array(roi)\n",
        "\troi = preprocess_input(roi)\n",
        "\n",
        "\t# update our proposals and bounding boxes lists\n",
        "\tproposals.append(roi)\n",
        "\tboxes.append((x, y, w, h))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKA1P6nXLZ02"
      },
      "source": [
        "# convert the proposals list into NumPy array and show its dimensions\n",
        "proposals = np.array(proposals)\n",
        "print(\"[INFO] proposal shape: {}\".format(proposals.shape))\n",
        "\n",
        "# classify each of the proposal ROIs using ResNet and then decode the\n",
        "# predictions\n",
        "print(\"[INFO] classifying proposals...\")\n",
        "preds = model.predict(proposals)\n",
        "preds = imagenet_utils.decode_predictions(preds, top=1)\n",
        "\n",
        "# initialize a dictionary which maps class labels (keys) to any\n",
        "# bounding box associated with that label (values)\n",
        "labels = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwdHQcxjLdBQ"
      },
      "source": [
        "# loop over the predictions\n",
        "for (i, p) in enumerate(preds):\n",
        "\t# grab the prediction information for the current region proposal\n",
        "\t(imagenetID, label, prob) = p[0]\n",
        "\n",
        "\t# only if the label filters are not empty *and* the label does not\n",
        "\t# exist in the list, then ignore it\n",
        "\tif labelFilters is not None and label not in labelFilters:\n",
        "\t\tcontinue\n",
        "\n",
        "\t# filter out weak detections by ensuring the predicted probability\n",
        "\t# is greater than the minimum probability\n",
        "\tif prob >= args[\"conf\"]:\n",
        "\t\t# grab the bounding box associated with the prediction and\n",
        "\t\t# convert the coordinates\n",
        "\t\t(x, y, w, h) = boxes[i]\n",
        "\t\tbox = (x, y, x + w, y + h)\n",
        "\n",
        "\t\t# grab the list of predictions for the label and add the\n",
        "\t\t# bounding box + probability to the list\n",
        "\t\tL = labels.get(label, [])\n",
        "\t\tL.append((box, prob))\n",
        "\t\tlabels[label] = L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c27SMs8gLg6m"
      },
      "source": [
        "# loop over the labels for each of detected objects in the image\n",
        "for label in labels.keys():\n",
        "\t# clone the original image so that we can draw on it\n",
        "\tprint(\"[INFO] showing results for '{}'\".format(label))\n",
        "\tclone = image.copy()\n",
        "\n",
        "\t# loop over all bounding boxes for the current label\n",
        "\tfor (box, prob) in labels[label]:\n",
        "\t\t# draw the bounding box on the image\n",
        "\t\t(startX, startY, endX, endY) = box\n",
        "\t\tcv2.rectangle(clone, (startX, startY), (endX, endY),\n",
        "\t\t\t(0, 255, 0), 2)\n",
        "\n",
        "\t# show the results *before* applying non-maxima suppression, then\n",
        "\t# clone the image again so we can display the results *after*\n",
        "\t# applying non-maxima suppression\n",
        "\tplt_imshow(\"Before\", clone)\n",
        "\tclone = image.copy()\n",
        "\n",
        "    # extract the bounding boxes and associated prediction\n",
        "\t# probabilities, then apply non-maxima suppression\n",
        "\tboxes = np.array([p[0] for p in labels[label]])\n",
        "\tproba = np.array([p[1] for p in labels[label]])\n",
        "\tboxes = non_max_suppression(boxes, proba)\n",
        "\n",
        "\t# loop over all bounding boxes that were kept after applying\n",
        "\t# non-maxima suppression\n",
        "\tfor (startX, startY, endX, endY) in boxes:\n",
        "\t\t# draw the bounding box and label on the image\n",
        "\t\tcv2.rectangle(clone, (startX, startY), (endX, endY),\n",
        "\t\t\t(0, 255, 0), 2)\n",
        "\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\t\tcv2.putText(clone, label, (startX, y),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
        "\n",
        "\t# show the output after apply non-maxima suppression\n",
        "\tplt_imshow(\"After\", clone)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}