{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_class_bbox_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girijeshcse/computervisionTFKeras/blob/main/object_detection/06%20multi_class_bbox_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlDoNUuKfnXY"
      },
      "source": [
        "# Multi-class object detection and bounding box regression with Keras, TensorFlow, and Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpuM4JQK-ecz"
      },
      "source": [
        "### Install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdtSckfk-hua"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd7KaOjSL8eS"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNRsM5JgLBCo"
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/multi-class-bounding-box-regression/multi-class-bbox-regression.zip\n",
        "!unzip -qq multi-class-bbox-regression.zip\n",
        "%cd multi-class-bbox-regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiymdqJCMN8l"
      },
      "source": [
        "## Blog Post Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HTU6O1DMRQA"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niBIFTezMCO5"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mimetypes\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXciPOrnMwZf"
      },
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWMvfFCRMxob"
      },
      "source": [
        "def plt_imshow(title, image):\n",
        "    # convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stuJPhF1MtHV"
      },
      "source": [
        "### Define our `Config` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1BtSEUEMiwD"
      },
      "source": [
        "class Config:\n",
        "    # define the base path to the input dataset and then use it to derive\n",
        "    # the path to the input images and annotation CSV files\n",
        "    BASE_PATH = \"dataset\"\n",
        "    IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
        "    ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"annotations\"])\n",
        "\n",
        "    # define the path to the base output directory\n",
        "    BASE_OUTPUT = \"output\"\n",
        "\n",
        "    # define the path to the output model, label binarizer, plots output\n",
        "    # directory, and testing image paths\n",
        "    MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\n",
        "    LB_PATH = os.path.sep.join([BASE_OUTPUT, \"lb.pickle\"])\n",
        "    PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
        "    TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n",
        "\n",
        "    # initialize our initial learning rate, number of epochs to train\n",
        "    # for, and the batch size\n",
        "    INIT_LR = 1e-4\n",
        "    NUM_EPOCHS = 20\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "# instantiate the config class\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhUaMKcYM2Zb"
      },
      "source": [
        "### Implementing our multi-class object detector training script with Keras and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkQf5-21Mzrd"
      },
      "source": [
        "# initialize the list of data (images), class labels, target bounding\n",
        "# box coordinates, and image paths\n",
        "print(\"[INFO] loading dataset...\")\n",
        "data = []\n",
        "labels = []\n",
        "bboxes = []\n",
        "imagePaths = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BPCg_k4NSgZ"
      },
      "source": [
        "# loop over all CSV files in the annotations directory\n",
        "for csvPath in paths.list_files(config.ANNOTS_PATH, validExts=(\".csv\")):\n",
        "\t# load the contents of the current CSV annotations file\n",
        "\trows = open(csvPath).read().strip().split(\"\\n\")\n",
        "\n",
        "\t# loop over the rows\n",
        "\tfor row in rows:\n",
        "\t\t# break the row into the filename, bounding box coordinates,\n",
        "\t\t# and class label\n",
        "\t\trow = row.split(\",\")\n",
        "\t\t(filename, startX, startY, endX, endY, label) = row\n",
        "\n",
        "\t\t# derive the path to the input image, load the image (in\n",
        "\t\t# OpenCV format), and grab its dimensions\n",
        "\t\timagePath = os.path.sep.join([config.IMAGES_PATH, label,\n",
        "\t\t\tfilename])\n",
        "\t\timage = cv2.imread(imagePath)\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\n",
        "\t\t# scale the bounding box coordinates relative to the spatial\n",
        "\t\t# dimensions of the input image\n",
        "\t\tstartX = float(startX) / w\n",
        "\t\tstartY = float(startY) / h\n",
        "\t\tendX = float(endX) / w\n",
        "\t\tendY = float(endY) / h\n",
        "\n",
        "\t\t# load the image and preprocess it\n",
        "\t\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\t\timage = img_to_array(image)\n",
        "\n",
        "\t\t# update our list of data, class labels, bounding boxes, and\n",
        "\t\t# image paths\n",
        "\t\tdata.append(image)\n",
        "\t\tlabels.append(label)\n",
        "\t\tbboxes.append((startX, startY, endX, endY))\n",
        "\t\timagePaths.append(imagePath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VydXIxyxNW1n"
      },
      "source": [
        "# convert the data, class labels, bounding boxes, and image paths to\n",
        "# NumPy arrays, scaling the input pixel intensities from the range\n",
        "# [0, 255] to [0, 1]\n",
        "data = np.array(data, dtype=\"float32\") / 255.0\n",
        "labels = np.array(labels)\n",
        "bboxes = np.array(bboxes, dtype=\"float32\")\n",
        "imagePaths = np.array(imagePaths)\n",
        "\n",
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# only there are only two labels in the dataset, then we need to use\n",
        "# Keras/TensorFlow's utility function as well\n",
        "if len(lb.classes_) == 2:\n",
        "\tlabels = to_categorical(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0TQW29hNy7_"
      },
      "source": [
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "split = train_test_split(data, labels, bboxes, imagePaths,\n",
        "\ttest_size=0.20, random_state=42)\n",
        "\n",
        "# unpack the data split\n",
        "(trainImages, testImages) = split[:2]\n",
        "(trainLabels, testLabels) = split[2:4]\n",
        "(trainBBoxes, testBBoxes) = split[4:6]\n",
        "(trainPaths, testPaths) = split[6:]\n",
        "\n",
        "# write the testing image paths to disk so that we can use then\n",
        "# when evaluating/testing our object detector\n",
        "print(\"[INFO] saving testing image paths...\")\n",
        "f = open(config.TEST_PATHS, \"w\")\n",
        "f.write(\"\\n\".join(testPaths))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MWMy2fqOjIf"
      },
      "source": [
        "# load the VGG16 network, ensuring the head FC layers are left off\n",
        "vgg = VGG16(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# freeze all VGG layers so they will *not* be updated during the\n",
        "# training process\n",
        "vgg.trainable = False\n",
        "\n",
        "# flatten the max-pooling output of VGG\n",
        "flatten = vgg.output\n",
        "flatten = Flatten()(flatten)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Yyt24tOovb"
      },
      "source": [
        "# construct a fully-connected layer header to output the predicted\n",
        "# bounding box coordinates\n",
        "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
        "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dense(4, activation=\"sigmoid\",\n",
        "\tname=\"bounding_box\")(bboxHead)\n",
        "\n",
        "# construct a second fully-connected layer head, this one to predict\n",
        "# the class label\n",
        "softmaxHead = Dense(512, activation=\"relu\")(flatten)\n",
        "softmaxHead = Dropout(0.5)(softmaxHead)\n",
        "softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n",
        "softmaxHead = Dropout(0.5)(softmaxHead)\n",
        "softmaxHead = Dense(len(lb.classes_), activation=\"softmax\",\n",
        "\tname=\"class_label\")(softmaxHead)\n",
        "\n",
        "# put together our model which accept an input image and then output\n",
        "# bounding box coordinates and a class label\n",
        "model = Model(\n",
        "\tinputs=vgg.input,\n",
        "\toutputs=(bboxHead, softmaxHead))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPXRmBRVOsxP"
      },
      "source": [
        "# define a dictionary to set the loss methods -- categorical\n",
        "# cross-entropy for the class label head and mean absolute error\n",
        "# for the bounding box head\n",
        "losses = {\n",
        "\t\"class_label\": \"categorical_crossentropy\",\n",
        "\t\"bounding_box\": \"mean_squared_error\",\n",
        "}\n",
        "\n",
        "# define a dictionary that specifies the weights per loss (both the\n",
        "# class label and bounding box outputs will receive equal weight)\n",
        "lossWeights = {\n",
        "\t\"class_label\": 1.0,\n",
        "\t\"bounding_box\": 1.0\n",
        "}\n",
        "\n",
        "# initialize the optimizer, compile the model, and show the model\n",
        "# summary\n",
        "opt = Adam(lr=config.INIT_LR)\n",
        "model.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"],\n",
        "    loss_weights=lossWeights)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBkkTBdKO-xq"
      },
      "source": [
        "# construct a dictionary for our target training outputs\n",
        "trainTargets = {\n",
        "\t\"class_label\": trainLabels,\n",
        "\t\"bounding_box\": trainBBoxes\n",
        "}\n",
        "\n",
        "# construct a second dictionary, this one for our target testing\n",
        "# outputs\n",
        "testTargets = {\n",
        "\t\"class_label\": testLabels,\n",
        "\t\"bounding_box\": testBBoxes\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1eMZN-rPDV5"
      },
      "source": [
        "# train the network for bounding box regression and class label\n",
        "# prediction\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit(\n",
        "\ttrainImages, trainTargets,\n",
        "\tvalidation_data=(testImages, testTargets),\n",
        "\tbatch_size=config.BATCH_SIZE,\n",
        "\tepochs=config.NUM_EPOCHS,\n",
        "\tverbose=1)\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving object detector model...\")\n",
        "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
        "\n",
        "# serialize the label binarizer to disk\n",
        "print(\"[INFO] saving label binarizer...\")\n",
        "f = open(config.LB_PATH, \"wb\")\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apwdrYugPkba"
      },
      "source": [
        "# plot the total loss, label loss, and bounding box loss\n",
        "lossNames = [\"loss\", \"class_label_loss\", \"bounding_box_loss\"]\n",
        "N = np.arange(0, config.NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
        "\n",
        "# loop over the loss names\n",
        "for (i, l) in enumerate(lossNames):\n",
        "\t# plot the loss for both the training and validation data\n",
        "\ttitle = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
        "\tax[i].set_title(title)\n",
        "\tax[i].set_xlabel(\"Epoch #\")\n",
        "\tax[i].set_ylabel(\"Loss\")\n",
        "\tax[i].plot(N, H.history[l], label=l)\n",
        "\tax[i].plot(N, H.history[\"val_\" + l], label=\"val_\" + l)\n",
        "\tax[i].legend()\n",
        "\n",
        "# display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCvEBgQNPoKi"
      },
      "source": [
        "# create a new figure for the accuracies\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"class_label_accuracy\"],\n",
        "\tlabel=\"class_label_train_acc\")\n",
        "plt.plot(N, H.history[\"val_class_label_accuracy\"],\n",
        "\tlabel=\"val_class_label_acc\")\n",
        "plt.title(\"Class Label Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTIe8OewQm6U"
      },
      "source": [
        "### Implementing the object detection prediction script with Keras and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDWQndWVRInb"
      },
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--input\", required=True,\n",
        "# \thelp=\"path to input image/text file of image filenames\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "    \"input\": \"output/test_paths.txt\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YTb-74MRKcu"
      },
      "source": [
        "# determine the input file type, but assume that we're working with\n",
        "# single input image\n",
        "filetype = mimetypes.guess_type(args[\"input\"])[0]\n",
        "imagePaths = [args[\"input\"]]\n",
        "\n",
        "# if the file type is a text file, then we need to process *multiple*\n",
        "# images\n",
        "if \"text/plain\" == filetype:\n",
        "\t# load the image paths in our testing file\n",
        "\timagePaths = open(args[\"input\"]).read().strip().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gy7MXebRQFh"
      },
      "source": [
        "# load our object detector and label binarizer from disk\n",
        "print(\"[INFO] loading object detector...\")\n",
        "model = load_model(config.MODEL_PATH)\n",
        "lb = pickle.loads(open(config.LB_PATH, \"rb\").read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guAtOcY_RZgj"
      },
      "source": [
        "# loop over the images that we'll be testing using our bounding box\n",
        "# regression model\n",
        "for imagePath in imagePaths:\n",
        "\t# load the input image (in Keras format) from disk and preprocess\n",
        "\t# it, scaling the pixel intensities to the range [0, 1]\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\timage = img_to_array(image) / 255.0\n",
        "\timage = np.expand_dims(image, axis=0)\n",
        "\n",
        "\t# predict the bounding box of the object along with the class\n",
        "\t# label\n",
        "\t(boxPreds, labelPreds) = model.predict(image)\n",
        "\t(startX, startY, endX, endY) = boxPreds[0]\n",
        "\n",
        "\t# determine the class label with the largest predicted\n",
        "\t# probability\n",
        "\ti = np.argmax(labelPreds, axis=1)\n",
        "\tlabel = lb.classes_[i][0]\n",
        "\n",
        "\t# load the input image (in OpenCV format), resize it such that it\n",
        "\t# fits on our screen, and grab its dimensions\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = imutils.resize(image, width=600)\n",
        "\t(h, w) = image.shape[:2]\n",
        "\n",
        "\t# scale the predicted bounding box coordinates based on the image\n",
        "\t# dimensions\n",
        "\tstartX = int(startX * w)\n",
        "\tstartY = int(startY * h)\n",
        "\tendX = int(endX * w)\n",
        "\tendY = int(endY * h)\n",
        "\n",
        "\t# draw the predicted bounding box and class label on the image\n",
        "\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\tcv2.putText(image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t0.65, (0, 255, 0), 2)\n",
        "\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
        "\t\t(0, 255, 0), 2)\n",
        "\n",
        "\t# show the output image\n",
        "\tplt_imshow(\"Output\", image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}